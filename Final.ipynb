{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57671561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before any code I'm going to import all dependancies\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models, layers, regularizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc1faf",
   "metadata": {},
   "source": [
    "# 1 Initial exploration\n",
    "### 1.1 Importing the dataset\n",
    "Before I can do anything with this dataset I need to import into this python file. To do that I will need to make it a pandas dataset as I find this easiest to manipulate and it has a function for importing csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4674dfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import a csv file that was sourced from https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee8169",
   "metadata": {},
   "source": [
    "### 1.2 Understanding the dataset\n",
    "Now the dataset is in a pandas dataframe I should be able to delve deeper into the database and find what properties I am looking at and other questions I may want to find out:\n",
    "\n",
    "* What statistics am I looking at (i.e. range of quality)\n",
    "* Is there any rows with missing values and are they importent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "455ff48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a519b6f",
   "metadata": {},
   "source": [
    "From looking at these statistics I can see that the quality of wine has a range of 5 (3-8) which I am a little disapointed as this limits the categories I aim for as if it was a range of 10 I could have 3 or 4 catogories but I am going to make this into a binary classification problem by saying 3-5 is bad quality wine represented by 0 and 6-8 is good quality wine represented by 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba7403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'] = df['quality'].map({3:0, 4:0, 5:0, 6:1, 7:1, 8:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae69b22",
   "metadata": {},
   "source": [
    "Now quality is binary I can do a binary classification ploblem I can start working towards finding the missing rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd55169b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9da99",
   "metadata": {},
   "source": [
    "I found out that the database is full so I can now do analysis without any unpredictable behavour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb6fe9",
   "metadata": {},
   "source": [
    "# 2 Making a prototype\n",
    "### 2.1 Final preperations\n",
    "Now that I have decided what to do the model based on I need to make all of the features into a tensor so there are no features that dominate due to it allowing larger numbers than others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4e33a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.528194</td>\n",
       "      <td>0.961576</td>\n",
       "      <td>-1.391037</td>\n",
       "      <td>-0.453077</td>\n",
       "      <td>-0.243630</td>\n",
       "      <td>-0.466047</td>\n",
       "      <td>-0.379014</td>\n",
       "      <td>0.558100</td>\n",
       "      <td>1.288240</td>\n",
       "      <td>-0.579025</td>\n",
       "      <td>-0.959946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.298454</td>\n",
       "      <td>1.966827</td>\n",
       "      <td>-1.391037</td>\n",
       "      <td>0.043403</td>\n",
       "      <td>0.223805</td>\n",
       "      <td>0.872365</td>\n",
       "      <td>0.624168</td>\n",
       "      <td>0.028252</td>\n",
       "      <td>-0.719708</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>-0.584594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.298454</td>\n",
       "      <td>1.296660</td>\n",
       "      <td>-1.185699</td>\n",
       "      <td>-0.169374</td>\n",
       "      <td>0.096323</td>\n",
       "      <td>-0.083643</td>\n",
       "      <td>0.228975</td>\n",
       "      <td>0.134222</td>\n",
       "      <td>-0.331073</td>\n",
       "      <td>-0.048074</td>\n",
       "      <td>-0.584594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.654339</td>\n",
       "      <td>-1.384011</td>\n",
       "      <td>1.483689</td>\n",
       "      <td>-0.453077</td>\n",
       "      <td>-0.264878</td>\n",
       "      <td>0.107558</td>\n",
       "      <td>0.411372</td>\n",
       "      <td>0.664069</td>\n",
       "      <td>-0.978798</td>\n",
       "      <td>-0.461036</td>\n",
       "      <td>-0.584594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.528194</td>\n",
       "      <td>0.961576</td>\n",
       "      <td>-1.391037</td>\n",
       "      <td>-0.453077</td>\n",
       "      <td>-0.243630</td>\n",
       "      <td>-0.466047</td>\n",
       "      <td>-0.379014</td>\n",
       "      <td>0.558100</td>\n",
       "      <td>1.288240</td>\n",
       "      <td>-0.579025</td>\n",
       "      <td>-0.959946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>-1.217415</td>\n",
       "      <td>0.403103</td>\n",
       "      <td>-0.980362</td>\n",
       "      <td>-0.382151</td>\n",
       "      <td>0.053829</td>\n",
       "      <td>1.541571</td>\n",
       "      <td>-0.075020</td>\n",
       "      <td>-0.978459</td>\n",
       "      <td>0.899605</td>\n",
       "      <td>-0.461036</td>\n",
       "      <td>0.072271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>-1.389721</td>\n",
       "      <td>0.123866</td>\n",
       "      <td>-0.877693</td>\n",
       "      <td>-0.240300</td>\n",
       "      <td>-0.541090</td>\n",
       "      <td>2.210777</td>\n",
       "      <td>0.137777</td>\n",
       "      <td>-0.861893</td>\n",
       "      <td>1.353012</td>\n",
       "      <td>0.600867</td>\n",
       "      <td>0.729136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>-1.159980</td>\n",
       "      <td>-0.099523</td>\n",
       "      <td>-0.723690</td>\n",
       "      <td>-0.169374</td>\n",
       "      <td>-0.243630</td>\n",
       "      <td>1.254769</td>\n",
       "      <td>-0.196617</td>\n",
       "      <td>-0.533387</td>\n",
       "      <td>0.705287</td>\n",
       "      <td>0.541872</td>\n",
       "      <td>0.541460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>-1.389721</td>\n",
       "      <td>0.654416</td>\n",
       "      <td>-0.775024</td>\n",
       "      <td>-0.382151</td>\n",
       "      <td>-0.264878</td>\n",
       "      <td>1.541571</td>\n",
       "      <td>-0.075020</td>\n",
       "      <td>-0.676446</td>\n",
       "      <td>1.676875</td>\n",
       "      <td>0.305894</td>\n",
       "      <td>-0.209243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>-1.332285</td>\n",
       "      <td>-1.216469</td>\n",
       "      <td>1.021680</td>\n",
       "      <td>0.752659</td>\n",
       "      <td>-0.434854</td>\n",
       "      <td>0.203159</td>\n",
       "      <td>-0.135818</td>\n",
       "      <td>-0.665849</td>\n",
       "      <td>0.510970</td>\n",
       "      <td>0.010921</td>\n",
       "      <td>0.541460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0         -0.528194          0.961576    -1.391037       -0.453077  -0.243630   \n",
       "1         -0.298454          1.966827    -1.391037        0.043403   0.223805   \n",
       "2         -0.298454          1.296660    -1.185699       -0.169374   0.096323   \n",
       "3          1.654339         -1.384011     1.483689       -0.453077  -0.264878   \n",
       "4         -0.528194          0.961576    -1.391037       -0.453077  -0.243630   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594      -1.217415          0.403103    -0.980362       -0.382151   0.053829   \n",
       "1595      -1.389721          0.123866    -0.877693       -0.240300  -0.541090   \n",
       "1596      -1.159980         -0.099523    -0.723690       -0.169374  -0.243630   \n",
       "1597      -1.389721          0.654416    -0.775024       -0.382151  -0.264878   \n",
       "1598      -1.332285         -1.216469     1.021680        0.752659  -0.434854   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "0               -0.466047             -0.379014  0.558100  1.288240   \n",
       "1                0.872365              0.624168  0.028252 -0.719708   \n",
       "2               -0.083643              0.228975  0.134222 -0.331073   \n",
       "3                0.107558              0.411372  0.664069 -0.978798   \n",
       "4               -0.466047             -0.379014  0.558100  1.288240   \n",
       "...                   ...                   ...       ...       ...   \n",
       "1594             1.541571             -0.075020 -0.978459  0.899605   \n",
       "1595             2.210777              0.137777 -0.861893  1.353012   \n",
       "1596             1.254769             -0.196617 -0.533387  0.705287   \n",
       "1597             1.541571             -0.075020 -0.676446  1.676875   \n",
       "1598             0.203159             -0.135818 -0.665849  0.510970   \n",
       "\n",
       "      sulphates   alcohol  \n",
       "0     -0.579025 -0.959946  \n",
       "1      0.128910 -0.584594  \n",
       "2     -0.048074 -0.584594  \n",
       "3     -0.461036 -0.584594  \n",
       "4     -0.579025 -0.959946  \n",
       "...         ...       ...  \n",
       "1594  -0.461036  0.072271  \n",
       "1595   0.600867  0.729136  \n",
       "1596   0.541872  0.541460  \n",
       "1597   0.305894 -0.209243  \n",
       "1598   0.010921  0.541460  \n",
       "\n",
       "[1599 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df['quality']\n",
    "df = df.drop('quality', axis=1)\n",
    "mean = df.mean(axis=0)\n",
    "std = df.std(axis=0)\n",
    "df -= mean\n",
    "df /= std\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8a6f78",
   "metadata": {},
   "source": [
    "Now the data is ready for deep learning it is time to actually implement the deep learning. First I need to split the data up into testing and training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9823dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is already split into feature and target\n",
    "X = df\n",
    "y = target\n",
    "#Now just split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "#Finally I need to split the test data into test and validation\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3344ca",
   "metadata": {},
   "source": [
    "### 2.2 Running the prototype\n",
    "Now that I have prepared the data for deep learning it is therefore ready to be processed. I am only going to do the initial stage as for now I just want to find out if this is going to work therefore I need it to beat the baseline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a39c6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the model\n",
    "def basicModel():\n",
    "    Model = models.Sequential()\n",
    "    Model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    Model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b97de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 876us/step - loss: 0.9680 - accuracy: 0.4829\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "Model = basicModel()\n",
    "result = Model.fit(X_train, y_train, epochs = 1, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692fdae",
   "metadata": {},
   "source": [
    "As this beats the baseline of 0.5 this means the model is statistical powerful and I can continue with this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ab2df",
   "metadata": {},
   "source": [
    "# 3 Improving a prototype\n",
    "### 3.1 Scalling up\n",
    "Now that the prototype shows that the model has statistical power I can work on trying to make it even better. The first step would be to improve the amount of epochs to run it through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7230e71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "24/24 [==============================] - 0s 724us/step - loss: 0.8472 - accuracy: 0.4554\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 0s 542us/step - loss: 0.8210 - accuracy: 0.4687\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 0s 330us/step - loss: 0.7982 - accuracy: 0.4871\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 0s 836us/step - loss: 0.7772 - accuracy: 0.4971\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 0s 431us/step - loss: 0.7583 - accuracy: 0.5129\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 0s 868us/step - loss: 0.7399 - accuracy: 0.5296\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 0s 802us/step - loss: 0.7229 - accuracy: 0.5463\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 0s 569us/step - loss: 0.7068 - accuracy: 0.5646\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 0s 501us/step - loss: 0.6920 - accuracy: 0.5838\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 0s 554us/step - loss: 0.6781 - accuracy: 0.6013\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 0s 656us/step - loss: 0.6655 - accuracy: 0.6180\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 0s 617us/step - loss: 0.6543 - accuracy: 0.6264\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 0s 563us/step - loss: 0.6437 - accuracy: 0.6255\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 0s 618us/step - loss: 0.6342 - accuracy: 0.6322\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 0s 492us/step - loss: 0.6255 - accuracy: 0.6447\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 0s 239us/step - loss: 0.6174 - accuracy: 0.6564\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 0s 390us/step - loss: 0.6104 - accuracy: 0.6589\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 0s 585us/step - loss: 0.6038 - accuracy: 0.6597\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 0s 653us/step - loss: 0.5976 - accuracy: 0.6606\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 0s 593us/step - loss: 0.5921 - accuracy: 0.6714\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 0s 325us/step - loss: 0.5870 - accuracy: 0.6781\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 0s 888us/step - loss: 0.5824 - accuracy: 0.6831\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 0s 569us/step - loss: 0.5782 - accuracy: 0.6914\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 0s 722us/step - loss: 0.5744 - accuracy: 0.6931\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 0s 694us/step - loss: 0.5708 - accuracy: 0.6931\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 0s 618us/step - loss: 0.5675 - accuracy: 0.7031\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 0s 579us/step - loss: 0.5645 - accuracy: 0.7023\n",
      "Epoch 28/50\n",
      "24/24 [==============================] - 0s 550us/step - loss: 0.5617 - accuracy: 0.7056\n",
      "Epoch 29/50\n",
      "24/24 [==============================] - 0s 188us/step - loss: 0.5593 - accuracy: 0.7106\n",
      "Epoch 30/50\n",
      "24/24 [==============================] - 0s 574us/step - loss: 0.5570 - accuracy: 0.7123\n",
      "Epoch 31/50\n",
      "24/24 [==============================] - 0s 548us/step - loss: 0.5547 - accuracy: 0.7123\n",
      "Epoch 32/50\n",
      "24/24 [==============================] - 0s 264us/step - loss: 0.5527 - accuracy: 0.7173\n",
      "Epoch 33/50\n",
      "24/24 [==============================] - 0s 968us/step - loss: 0.5507 - accuracy: 0.7181\n",
      "Epoch 34/50\n",
      "24/24 [==============================] - 0s 280us/step - loss: 0.5491 - accuracy: 0.7198\n",
      "Epoch 35/50\n",
      "24/24 [==============================] - 0s 860us/step - loss: 0.5475 - accuracy: 0.7223\n",
      "Epoch 36/50\n",
      "24/24 [==============================] - 0s 590us/step - loss: 0.5461 - accuracy: 0.7239\n",
      "Epoch 37/50\n",
      "24/24 [==============================] - 0s 462us/step - loss: 0.5447 - accuracy: 0.7248\n",
      "Epoch 38/50\n",
      "24/24 [==============================] - 0s 725us/step - loss: 0.5435 - accuracy: 0.7256\n",
      "Epoch 39/50\n",
      "24/24 [==============================] - 0s 590us/step - loss: 0.5423 - accuracy: 0.7248\n",
      "Epoch 40/50\n",
      "24/24 [==============================] - 0s 634us/step - loss: 0.5412 - accuracy: 0.7256\n",
      "Epoch 41/50\n",
      "24/24 [==============================] - 0s 585us/step - loss: 0.5403 - accuracy: 0.7264\n",
      "Epoch 42/50\n",
      "24/24 [==============================] - 0s 597us/step - loss: 0.5394 - accuracy: 0.7289\n",
      "Epoch 43/50\n",
      "24/24 [==============================] - 0s 576us/step - loss: 0.5386 - accuracy: 0.7298\n",
      "Epoch 44/50\n",
      "24/24 [==============================] - 0s 562us/step - loss: 0.5379 - accuracy: 0.7281\n",
      "Epoch 45/50\n",
      "24/24 [==============================] - 0s 424us/step - loss: 0.5371 - accuracy: 0.7306\n",
      "Epoch 46/50\n",
      "24/24 [==============================] - 0s 877us/step - loss: 0.5365 - accuracy: 0.7289\n",
      "Epoch 47/50\n",
      "24/24 [==============================] - 0s 466us/step - loss: 0.5358 - accuracy: 0.7314\n",
      "Epoch 48/50\n",
      "24/24 [==============================] - 0s 841us/step - loss: 0.5352 - accuracy: 0.7323\n",
      "Epoch 49/50\n",
      "24/24 [==============================] - 0s 611us/step - loss: 0.5347 - accuracy: 0.7331\n",
      "Epoch 50/50\n",
      "24/24 [==============================] - 0s 725us/step - loss: 0.5342 - accuracy: 0.7323\n"
     ]
    }
   ],
   "source": [
    "Model = basicModel()\n",
    "result = Model.fit(X_train, y_train, epochs = 50, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a8acf",
   "metadata": {},
   "source": [
    "At 36 epochs the algorithm starts to overfit so for now I am going to run the algorithm for 36 epochs. Now I have worked out how many epochs I need to run the model on it is time to add layers to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "033c8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the model\n",
    "def layeredModel():\n",
    "    Model = models.Sequential()\n",
    "    Model.add(layers.Dense(4, activation='relu'))\n",
    "    Model.add(layers.Dense(4, activation='relu'))\n",
    "    Model.add(layers.Dense(4, activation='relu'))\n",
    "    Model.add(layers.Dense(4, activation='relu'))\n",
    "    Model.add(layers.Dense(4, activation='relu'))\n",
    "    Model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    Model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f64cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "24/24 [==============================] - 1s 493us/step - loss: 0.6891 - accuracy: 0.5471\n",
      "Epoch 2/36\n",
      "24/24 [==============================] - 0s 582us/step - loss: 0.6840 - accuracy: 0.5480\n",
      "Epoch 3/36\n",
      "24/24 [==============================] - 0s 636us/step - loss: 0.6788 - accuracy: 0.5480\n",
      "Epoch 4/36\n",
      "24/24 [==============================] - 0s 678us/step - loss: 0.6726 - accuracy: 0.5480\n",
      "Epoch 5/36\n",
      "24/24 [==============================] - 0s 701us/step - loss: 0.6657 - accuracy: 0.5480\n",
      "Epoch 6/36\n",
      "24/24 [==============================] - 0s 769us/step - loss: 0.6586 - accuracy: 0.5480\n",
      "Epoch 7/36\n",
      "24/24 [==============================] - 0s 660us/step - loss: 0.6512 - accuracy: 0.5480\n",
      "Epoch 8/36\n",
      "24/24 [==============================] - 0s 566us/step - loss: 0.6433 - accuracy: 0.5480\n",
      "Epoch 9/36\n",
      "24/24 [==============================] - 0s 511us/step - loss: 0.6352 - accuracy: 0.5480\n",
      "Epoch 10/36\n",
      "24/24 [==============================] - 0s 735us/step - loss: 0.6277 - accuracy: 0.6339\n",
      "Epoch 11/36\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.6211 - accuracy: 0.7131\n",
      "Epoch 12/36\n",
      "24/24 [==============================] - 0s 564us/step - loss: 0.6150 - accuracy: 0.7106\n",
      "Epoch 13/36\n",
      "24/24 [==============================] - 0s 446us/step - loss: 0.6097 - accuracy: 0.7156\n",
      "Epoch 14/36\n",
      "24/24 [==============================] - 0s 914us/step - loss: 0.6054 - accuracy: 0.7173\n",
      "Epoch 15/36\n",
      "24/24 [==============================] - 0s 510us/step - loss: 0.6018 - accuracy: 0.7214\n",
      "Epoch 16/36\n",
      "24/24 [==============================] - 0s 448us/step - loss: 0.5983 - accuracy: 0.7214\n",
      "Epoch 17/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5947 - accuracy: 0.7273\n",
      "Epoch 18/36\n",
      "24/24 [==============================] - 0s 652us/step - loss: 0.5915 - accuracy: 0.7314\n",
      "Epoch 19/36\n",
      "24/24 [==============================] - 0s 298us/step - loss: 0.5886 - accuracy: 0.7314\n",
      "Epoch 20/36\n",
      "24/24 [==============================] - 0s 483us/step - loss: 0.5855 - accuracy: 0.7348\n",
      "Epoch 21/36\n",
      "24/24 [==============================] - 0s 816us/step - loss: 0.5826 - accuracy: 0.7339\n",
      "Epoch 22/36\n",
      "24/24 [==============================] - 0s 876us/step - loss: 0.5805 - accuracy: 0.7348\n",
      "Epoch 23/36\n",
      "24/24 [==============================] - 0s 433us/step - loss: 0.5778 - accuracy: 0.7273\n",
      "Epoch 24/36\n",
      "24/24 [==============================] - 0s 865us/step - loss: 0.5758 - accuracy: 0.7306\n",
      "Epoch 25/36\n",
      "24/24 [==============================] - 0s 679us/step - loss: 0.5734 - accuracy: 0.7289\n",
      "Epoch 26/36\n",
      "24/24 [==============================] - 0s 638us/step - loss: 0.5714 - accuracy: 0.7298\n",
      "Epoch 27/36\n",
      "24/24 [==============================] - 0s 655us/step - loss: 0.5696 - accuracy: 0.7314\n",
      "Epoch 28/36\n",
      "24/24 [==============================] - 0s 673us/step - loss: 0.5675 - accuracy: 0.7306\n",
      "Epoch 29/36\n",
      "24/24 [==============================] - 0s 561us/step - loss: 0.5654 - accuracy: 0.7289\n",
      "Epoch 30/36\n",
      "24/24 [==============================] - 0s 747us/step - loss: 0.5629 - accuracy: 0.7314\n",
      "Epoch 31/36\n",
      "24/24 [==============================] - 0s 882us/step - loss: 0.5609 - accuracy: 0.7273\n",
      "Epoch 32/36\n",
      "24/24 [==============================] - 0s 502us/step - loss: 0.5586 - accuracy: 0.7348\n",
      "Epoch 33/36\n",
      "24/24 [==============================] - 0s 440us/step - loss: 0.5573 - accuracy: 0.7323\n",
      "Epoch 34/36\n",
      "24/24 [==============================] - 0s 880us/step - loss: 0.5553 - accuracy: 0.7348\n",
      "Epoch 35/36\n",
      "24/24 [==============================] - 0s 552us/step - loss: 0.5537 - accuracy: 0.7356\n",
      "Epoch 36/36\n",
      "24/24 [==============================] - 0s 436us/step - loss: 0.5526 - accuracy: 0.7331\n"
     ]
    }
   ],
   "source": [
    "Model = layeredModel()\n",
    "result = Model.fit(X_train, y_train, epochs = 36, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6166a7f",
   "metadata": {},
   "source": [
    "| hidden_layers | accuracy |\n",
    "|---------------|----------|\n",
    "| 0             | 0.7248   |\n",
    "| 1             | 0.7373   |\n",
    "| 2             | 0.7423   |\n",
    "| 3             | 0.7515   |\n",
    "| 4             | 0.7415   |\n",
    "| 5             | 0.7373   |\n",
    "\n",
    "As the model starts to overfit at 4 hidden layers I am going to run the real model at 3 hidden layers. Now it is ready to work out how dense the layers of the model should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2da3a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denseModel():\n",
    "    Model = models.Sequential()\n",
    "    Model.add(layers.Dense(2048, activation='relu'))\n",
    "    Model.add(layers.Dense(2048, activation='relu'))\n",
    "    Model.add(layers.Dense(2048, activation='relu'))\n",
    "    Model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    Model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccd7f504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "24/24 [==============================] - 3s 93ms/step - loss: 0.8351 - accuracy: 0.6564\n",
      "Epoch 2/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.5465 - accuracy: 0.7256\n",
      "Epoch 3/36\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.5249 - accuracy: 0.7431\n",
      "Epoch 4/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.4979 - accuracy: 0.7590\n",
      "Epoch 5/36\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.5008 - accuracy: 0.7673\n",
      "Epoch 6/36\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.4735 - accuracy: 0.7706\n",
      "Epoch 7/36\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.4637 - accuracy: 0.7756\n",
      "Epoch 8/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.4441 - accuracy: 0.7932\n",
      "Epoch 9/36\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.4382 - accuracy: 0.7898\n",
      "Epoch 10/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.4062 - accuracy: 0.8140\n",
      "Epoch 11/36\n",
      "24/24 [==============================] - 2s 96ms/step - loss: 0.4014 - accuracy: 0.8198\n",
      "Epoch 12/36\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.3823 - accuracy: 0.8148\n",
      "Epoch 13/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.3523 - accuracy: 0.8382\n",
      "Epoch 14/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.3538 - accuracy: 0.8390\n",
      "Epoch 15/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.3224 - accuracy: 0.8549\n",
      "Epoch 16/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.3064 - accuracy: 0.8599\n",
      "Epoch 17/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2941 - accuracy: 0.8716\n",
      "Epoch 18/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2731 - accuracy: 0.8791\n",
      "Epoch 19/36\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2712 - accuracy: 0.8824\n",
      "Epoch 20/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2532 - accuracy: 0.8916\n",
      "Epoch 21/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2347 - accuracy: 0.8991\n",
      "Epoch 22/36\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2197 - accuracy: 0.9141\n",
      "Epoch 23/36\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.1977 - accuracy: 0.9208\n",
      "Epoch 24/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.1837 - accuracy: 0.9274\n",
      "Epoch 25/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.1913 - accuracy: 0.9199\n",
      "Epoch 26/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.1887 - accuracy: 0.9224\n",
      "Epoch 27/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.1604 - accuracy: 0.9374\n",
      "Epoch 28/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.1691 - accuracy: 0.9383\n",
      "Epoch 29/36\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.1310 - accuracy: 0.9450\n",
      "Epoch 30/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.1480 - accuracy: 0.9366\n",
      "Epoch 31/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.1314 - accuracy: 0.9500\n",
      "Epoch 32/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.1352 - accuracy: 0.9525\n",
      "Epoch 33/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.1367 - accuracy: 0.9491\n",
      "Epoch 34/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.1119 - accuracy: 0.9616\n",
      "Epoch 35/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.1282 - accuracy: 0.9575\n",
      "Epoch 36/36\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.0994 - accuracy: 0.9625\n"
     ]
    }
   ],
   "source": [
    "Model = denseModel()\n",
    "result = Model.fit(X_train, y_train, epochs = 36, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab5e581",
   "metadata": {},
   "source": [
    "| Density  | accuracy |\n",
    "|----------|----------|\n",
    "| 4        | 0.7581   |\n",
    "| 8        | 0.7656   |\n",
    "| 16       | 0.7665   |\n",
    "| 32       | 0.8190   |\n",
    "| 64       | 0.8791   |\n",
    "| 128      | 0.9099   |\n",
    "| 256      | 0.9358   |\n",
    "| 512      | 0.9700   |\n",
    "| 1024     | 0.9658   |\n",
    "| 2048     | 0.9500   |\n",
    "\n",
    "As the model starts to overfit the training data when the density is 1024 I am going to make a model that has layers with density of 512. Now I have scaled up the model I am going to regulize to get the perfect model.\n",
    "### 3.2 Regularizing and tuning hyperparameters\n",
    "Now we have scaled the model up we need to refine the model. This will require using the validation data as this isn't about trying to overfit the training data anymore I will need to assess the model on unseen data to get a better idea of how as accurate the model is. The first step to do this is changing the architecture of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7953a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def archModel():\n",
    "    Model = models.Sequential()\n",
    "    Model.add(layers.Dense(512, activation='relu'))\n",
    "    Model.add(layers.Dense(512, activation='relu'))\n",
    "    Model.add(layers.Dense(1024, activation='relu'))\n",
    "    Model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    Model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79f7accc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "24/24 [==============================] - 1s 14ms/step - loss: 0.5941 - accuracy: 0.6922 - val_loss: 0.5234 - val_accuracy: 0.7600\n",
      "Epoch 2/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5176 - accuracy: 0.7456 - val_loss: 0.4997 - val_accuracy: 0.7500\n",
      "Epoch 3/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5092 - accuracy: 0.7648 - val_loss: 0.4919 - val_accuracy: 0.7600\n",
      "Epoch 4/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4877 - accuracy: 0.7715 - val_loss: 0.5233 - val_accuracy: 0.7450\n",
      "Epoch 5/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4609 - accuracy: 0.7790 - val_loss: 0.5130 - val_accuracy: 0.7550\n",
      "Epoch 6/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4526 - accuracy: 0.7848 - val_loss: 0.4789 - val_accuracy: 0.7700\n",
      "Epoch 7/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4379 - accuracy: 0.7940 - val_loss: 0.5018 - val_accuracy: 0.7800\n",
      "Epoch 8/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4141 - accuracy: 0.8107 - val_loss: 0.4787 - val_accuracy: 0.7700\n",
      "Epoch 9/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3985 - accuracy: 0.8265 - val_loss: 0.4983 - val_accuracy: 0.7800\n",
      "Epoch 10/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3842 - accuracy: 0.8307 - val_loss: 0.5268 - val_accuracy: 0.7550\n",
      "Epoch 11/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3670 - accuracy: 0.8382 - val_loss: 0.5602 - val_accuracy: 0.7900\n",
      "Epoch 12/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3397 - accuracy: 0.8499 - val_loss: 0.5730 - val_accuracy: 0.7700\n",
      "Epoch 13/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3268 - accuracy: 0.8574 - val_loss: 0.5729 - val_accuracy: 0.7600\n",
      "Epoch 14/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3162 - accuracy: 0.8599 - val_loss: 0.5983 - val_accuracy: 0.7900\n",
      "Epoch 15/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2914 - accuracy: 0.8741 - val_loss: 0.5354 - val_accuracy: 0.7900\n",
      "Epoch 16/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2845 - accuracy: 0.8807 - val_loss: 0.5672 - val_accuracy: 0.7900\n",
      "Epoch 17/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2598 - accuracy: 0.8916 - val_loss: 0.6278 - val_accuracy: 0.7400\n",
      "Epoch 18/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2377 - accuracy: 0.9074 - val_loss: 0.7279 - val_accuracy: 0.7500\n",
      "Epoch 19/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2394 - accuracy: 0.8899 - val_loss: 0.6325 - val_accuracy: 0.7900\n",
      "Epoch 20/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2068 - accuracy: 0.9116 - val_loss: 0.6923 - val_accuracy: 0.8150\n",
      "Epoch 21/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2111 - accuracy: 0.9174 - val_loss: 0.6247 - val_accuracy: 0.8000\n",
      "Epoch 22/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2079 - accuracy: 0.9116 - val_loss: 0.7294 - val_accuracy: 0.8000\n",
      "Epoch 23/36\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1583 - accuracy: 0.9341 - val_loss: 0.8855 - val_accuracy: 0.7750\n",
      "Epoch 24/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1663 - accuracy: 0.9291 - val_loss: 0.8535 - val_accuracy: 0.8000\n",
      "Epoch 25/36\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1551 - accuracy: 0.9391 - val_loss: 0.9347 - val_accuracy: 0.7800\n",
      "Epoch 26/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1428 - accuracy: 0.9366 - val_loss: 1.0148 - val_accuracy: 0.7750\n",
      "Epoch 27/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1510 - accuracy: 0.9374 - val_loss: 0.8813 - val_accuracy: 0.7900\n",
      "Epoch 28/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1289 - accuracy: 0.9541 - val_loss: 0.9438 - val_accuracy: 0.7850\n",
      "Epoch 29/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1215 - accuracy: 0.9525 - val_loss: 1.0176 - val_accuracy: 0.7600\n",
      "Epoch 30/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1169 - accuracy: 0.9525 - val_loss: 1.0790 - val_accuracy: 0.7850\n",
      "Epoch 31/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1075 - accuracy: 0.9608 - val_loss: 0.9861 - val_accuracy: 0.8050\n",
      "Epoch 32/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1103 - accuracy: 0.9600 - val_loss: 1.1484 - val_accuracy: 0.7650\n",
      "Epoch 33/36\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0861 - accuracy: 0.9675 - val_loss: 1.2444 - val_accuracy: 0.7550\n",
      "Epoch 34/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0866 - accuracy: 0.9725 - val_loss: 1.2645 - val_accuracy: 0.8150\n",
      "Epoch 35/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0992 - accuracy: 0.9591 - val_loss: 1.1381 - val_accuracy: 0.7800\n",
      "Epoch 36/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0725 - accuracy: 0.9725 - val_loss: 1.3918 - val_accuracy: 0.7700\n"
     ]
    }
   ],
   "source": [
    "Model = archModel()\n",
    "result = Model.fit(X_train, y_train, epochs = 36, batch_size=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1867e",
   "metadata": {},
   "source": [
    "| architecture | accuracy | val_accuracy |\n",
    "|--------------|----------|--------------|\n",
    "| 512,256,256  | 0.9483   | 0.7500       |\n",
    "| 256,512,256  | 0.9633   | 0.7600       |\n",
    "| 256,256,512  | 0.9508   | 0.7700       |\n",
    "| 512,512,256  | 0.9633   | 0.7500       |\n",
    "| 512,256,512  | 0.9641   | 0.7600       |\n",
    "| 256,512,512  | 0.9650   | 0.7200       |\n",
    "| 512,512,512  | 0.9650   | 0.7700       |\n",
    "| 1024,512,512 | 0.9700   | 0.7600       |\n",
    "| 512,1024,512 | 0.9683   | 0.7500       |\n",
    "| 512,512,1024 | 0.9691   | 0.7400       |\n",
    "\n",
    "All of these architectures have similar validation accuracy but it looks like the architecture that does the best is the one that has all three layers having the density of 512. Now we have worked out what the architecture of our model we can start adding dropout to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e44aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropoutModel():\n",
    "    Model = models.Sequential()\n",
    "    Model.add(layers.Dense(512, activation='relu'))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(512, activation='relu'))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(512, activation='relu'))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    Model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "899a521e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "24/24 [==============================] - 1s 10ms/step - loss: 0.5743 - accuracy: 0.7023 - val_loss: 0.5035 - val_accuracy: 0.7550\n",
      "Epoch 2/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.5322 - accuracy: 0.7440 - val_loss: 0.4937 - val_accuracy: 0.7500\n",
      "Epoch 3/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.5074 - accuracy: 0.7556 - val_loss: 0.5028 - val_accuracy: 0.7400\n",
      "Epoch 4/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.5044 - accuracy: 0.7490 - val_loss: 0.4974 - val_accuracy: 0.7300\n",
      "Epoch 5/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.5004 - accuracy: 0.7506 - val_loss: 0.4950 - val_accuracy: 0.7650\n",
      "Epoch 6/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4863 - accuracy: 0.7648 - val_loss: 0.5042 - val_accuracy: 0.7500\n",
      "Epoch 7/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4704 - accuracy: 0.7748 - val_loss: 0.5113 - val_accuracy: 0.7500\n",
      "Epoch 8/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4628 - accuracy: 0.7781 - val_loss: 0.5163 - val_accuracy: 0.7550\n",
      "Epoch 9/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4570 - accuracy: 0.7790 - val_loss: 0.4956 - val_accuracy: 0.7750\n",
      "Epoch 10/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4449 - accuracy: 0.7948 - val_loss: 0.4938 - val_accuracy: 0.7500\n",
      "Epoch 11/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4331 - accuracy: 0.7932 - val_loss: 0.5179 - val_accuracy: 0.7400\n",
      "Epoch 12/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4196 - accuracy: 0.8048 - val_loss: 0.5149 - val_accuracy: 0.7600\n",
      "Epoch 13/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4112 - accuracy: 0.8107 - val_loss: 0.5321 - val_accuracy: 0.7800\n",
      "Epoch 14/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4053 - accuracy: 0.8073 - val_loss: 0.4962 - val_accuracy: 0.7700\n",
      "Epoch 15/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.4000 - accuracy: 0.8157 - val_loss: 0.5339 - val_accuracy: 0.7500\n",
      "Epoch 16/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3818 - accuracy: 0.8265 - val_loss: 0.5356 - val_accuracy: 0.7500\n",
      "Epoch 17/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3751 - accuracy: 0.8257 - val_loss: 0.4954 - val_accuracy: 0.7700\n",
      "Epoch 18/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3629 - accuracy: 0.8365 - val_loss: 0.5244 - val_accuracy: 0.7650\n",
      "Epoch 19/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3593 - accuracy: 0.8382 - val_loss: 0.5014 - val_accuracy: 0.7850\n",
      "Epoch 20/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3410 - accuracy: 0.8465 - val_loss: 0.5192 - val_accuracy: 0.7700\n",
      "Epoch 21/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3335 - accuracy: 0.8482 - val_loss: 0.5987 - val_accuracy: 0.7750\n",
      "Epoch 22/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3308 - accuracy: 0.8616 - val_loss: 0.5116 - val_accuracy: 0.8000\n",
      "Epoch 23/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3295 - accuracy: 0.8632 - val_loss: 0.5833 - val_accuracy: 0.7700\n",
      "Epoch 24/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3161 - accuracy: 0.8549 - val_loss: 0.5889 - val_accuracy: 0.7350\n",
      "Epoch 25/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.3030 - accuracy: 0.8699 - val_loss: 0.6252 - val_accuracy: 0.7750\n",
      "Epoch 26/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2987 - accuracy: 0.8666 - val_loss: 0.5278 - val_accuracy: 0.7800\n",
      "Epoch 27/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2764 - accuracy: 0.8782 - val_loss: 0.6440 - val_accuracy: 0.7550\n",
      "Epoch 28/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2908 - accuracy: 0.8716 - val_loss: 0.5851 - val_accuracy: 0.7650\n",
      "Epoch 29/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2614 - accuracy: 0.8857 - val_loss: 0.6264 - val_accuracy: 0.7450\n",
      "Epoch 30/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2751 - accuracy: 0.8799 - val_loss: 0.5757 - val_accuracy: 0.7900\n",
      "Epoch 31/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2505 - accuracy: 0.8841 - val_loss: 0.6629 - val_accuracy: 0.7800\n",
      "Epoch 32/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2671 - accuracy: 0.8782 - val_loss: 0.6771 - val_accuracy: 0.7800\n",
      "Epoch 33/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2403 - accuracy: 0.9024 - val_loss: 0.6977 - val_accuracy: 0.7850\n",
      "Epoch 34/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2413 - accuracy: 0.8999 - val_loss: 0.7202 - val_accuracy: 0.7350\n",
      "Epoch 35/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2366 - accuracy: 0.9091 - val_loss: 0.7225 - val_accuracy: 0.8000\n",
      "Epoch 36/36\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2192 - accuracy: 0.9116 - val_loss: 0.7296 - val_accuracy: 0.7450\n"
     ]
    }
   ],
   "source": [
    "Model = dropoutModel()\n",
    "result = Model.fit(X_train, y_train, epochs = 36, batch_size=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783d125",
   "metadata": {},
   "source": [
    "| dropout | accuracy | val_accuracy |\n",
    "|---------|----------|--------------|\n",
    "| 0       | 0.9591   | 0.7550       |\n",
    "| 0.2     | 0.8941   | 0.7700       |\n",
    "| 0.3     | 0.8532   | 0.7250       |\n",
    "| 0.5     | 0.7915   | 0.7650       |\n",
    "| 0.25    | 0.8649   | 0.7500       |\n",
    "| 0.15    | 0.9141   | 0.7300       |\n",
    "\n",
    "From this table we can see the dropout that gives us the highest validation accuracy is 0.2. Therefore I am going to use this to make my real model. The last thing I need to do in order to have a fully optimized model is to add regularizations to my model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24e20539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossModel():\n",
    "    Model = models.Sequential()\n",
    "    Model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.002)))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.002)))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.002)))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    Model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31e70622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "24/24 [==============================] - 1s 13ms/step - loss: 1.3898 - accuracy: 0.7081 - val_loss: 1.1781 - val_accuracy: 0.7600\n",
      "Epoch 2/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 1.0934 - accuracy: 0.7373 - val_loss: 0.9952 - val_accuracy: 0.7400\n",
      "Epoch 3/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.9104 - accuracy: 0.7565 - val_loss: 0.8361 - val_accuracy: 0.7350\n",
      "Epoch 4/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.7943 - accuracy: 0.7565 - val_loss: 0.7497 - val_accuracy: 0.7650\n",
      "Epoch 5/36\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.7148 - accuracy: 0.7573 - val_loss: 0.7057 - val_accuracy: 0.7350\n",
      "Epoch 6/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6652 - accuracy: 0.7665 - val_loss: 0.6758 - val_accuracy: 0.7350\n",
      "Epoch 7/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6317 - accuracy: 0.7540 - val_loss: 0.6341 - val_accuracy: 0.7500\n",
      "Epoch 8/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6030 - accuracy: 0.7631 - val_loss: 0.6143 - val_accuracy: 0.7500\n",
      "Epoch 9/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5839 - accuracy: 0.7681 - val_loss: 0.5875 - val_accuracy: 0.7400\n",
      "Epoch 10/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5731 - accuracy: 0.7548 - val_loss: 0.5746 - val_accuracy: 0.7600\n",
      "Epoch 11/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5630 - accuracy: 0.7748 - val_loss: 0.5811 - val_accuracy: 0.7350\n",
      "Epoch 12/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5497 - accuracy: 0.7681 - val_loss: 0.5808 - val_accuracy: 0.7400\n",
      "Epoch 13/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5485 - accuracy: 0.7790 - val_loss: 0.5537 - val_accuracy: 0.7400\n",
      "Epoch 14/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5251 - accuracy: 0.7832 - val_loss: 0.5887 - val_accuracy: 0.7600\n",
      "Epoch 15/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5231 - accuracy: 0.7898 - val_loss: 0.5578 - val_accuracy: 0.7550\n",
      "Epoch 16/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5109 - accuracy: 0.7957 - val_loss: 0.5563 - val_accuracy: 0.7800\n",
      "Epoch 17/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5126 - accuracy: 0.7798 - val_loss: 0.5762 - val_accuracy: 0.7650\n",
      "Epoch 18/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5057 - accuracy: 0.7948 - val_loss: 0.5547 - val_accuracy: 0.7300\n",
      "Epoch 19/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5051 - accuracy: 0.7957 - val_loss: 0.5591 - val_accuracy: 0.7500\n",
      "Epoch 20/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4950 - accuracy: 0.7923 - val_loss: 0.5463 - val_accuracy: 0.7400\n",
      "Epoch 21/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4999 - accuracy: 0.7890 - val_loss: 0.5598 - val_accuracy: 0.7550\n",
      "Epoch 22/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4945 - accuracy: 0.7982 - val_loss: 0.5723 - val_accuracy: 0.7550\n",
      "Epoch 23/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4804 - accuracy: 0.8065 - val_loss: 0.5475 - val_accuracy: 0.7550\n",
      "Epoch 24/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4808 - accuracy: 0.8090 - val_loss: 0.5438 - val_accuracy: 0.7800\n",
      "Epoch 25/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4797 - accuracy: 0.8023 - val_loss: 0.5472 - val_accuracy: 0.7750\n",
      "Epoch 26/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4692 - accuracy: 0.8098 - val_loss: 0.5727 - val_accuracy: 0.7650\n",
      "Epoch 27/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4659 - accuracy: 0.8115 - val_loss: 0.5350 - val_accuracy: 0.7700\n",
      "Epoch 28/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4705 - accuracy: 0.8148 - val_loss: 0.5698 - val_accuracy: 0.7700\n",
      "Epoch 29/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4552 - accuracy: 0.8232 - val_loss: 0.5469 - val_accuracy: 0.7700\n",
      "Epoch 30/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4551 - accuracy: 0.8265 - val_loss: 0.5805 - val_accuracy: 0.7650\n",
      "Epoch 31/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4486 - accuracy: 0.8332 - val_loss: 0.5841 - val_accuracy: 0.7900\n",
      "Epoch 32/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4509 - accuracy: 0.8165 - val_loss: 0.5448 - val_accuracy: 0.7700\n",
      "Epoch 33/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4489 - accuracy: 0.8340 - val_loss: 0.5598 - val_accuracy: 0.7850\n",
      "Epoch 34/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4387 - accuracy: 0.8265 - val_loss: 0.5589 - val_accuracy: 0.8000\n",
      "Epoch 35/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4370 - accuracy: 0.8332 - val_loss: 0.5509 - val_accuracy: 0.7950\n",
      "Epoch 36/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4256 - accuracy: 0.8440 - val_loss: 0.6399 - val_accuracy: 0.7650\n"
     ]
    }
   ],
   "source": [
    "Model = lossModel()\n",
    "result = Model.fit(X_train, y_train, epochs = 36, batch_size=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b30b666",
   "metadata": {},
   "source": [
    "| Regulation | amount | accuracy | val_accuracy |\n",
    "|------------|--------|----------|--------------|\n",
    "| None       | 0      | 0.9241   | 0.7300       |\n",
    "| L1         | 0.001  | 0.7573   | 0.7500       |\n",
    "| L2         | 0.001  | 0.8390   | 0.7850       |\n",
    "| L1, L2     | 0.001  | 0.7389   | 0.7350       |\n",
    "| L2         | 0.002  | 0.8440   | 0.7650       |\n",
    "\n",
    "This table shows that the best regularizers is L2 with the weight being 0.001 as this produces the highest val_accuracy at 0.7850.\n",
    "\n",
    "### 3.3 Running the final network\n",
    "Now I have scaled my model up and regularized it I need to run my final network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b4685f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalModel():\n",
    "    Model = models.Sequential()\n",
    "    Model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    Model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f074cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "24/24 [==============================] - 1s 13ms/step - loss: 1.3887 - accuracy: 0.7014 - val_loss: 1.1688 - val_accuracy: 0.7400\n",
      "Epoch 2/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 1.0812 - accuracy: 0.7406 - val_loss: 0.9612 - val_accuracy: 0.7550\n",
      "Epoch 3/36\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.9003 - accuracy: 0.7556 - val_loss: 0.8474 - val_accuracy: 0.7600\n",
      "Epoch 4/36\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.7919 - accuracy: 0.7440 - val_loss: 0.7420 - val_accuracy: 0.7600\n",
      "Epoch 5/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.7172 - accuracy: 0.7581 - val_loss: 0.6773 - val_accuracy: 0.7650\n",
      "Epoch 6/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6748 - accuracy: 0.7598 - val_loss: 0.6382 - val_accuracy: 0.7550\n",
      "Epoch 7/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6348 - accuracy: 0.7590 - val_loss: 0.6215 - val_accuracy: 0.7550\n",
      "Epoch 8/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6048 - accuracy: 0.7615 - val_loss: 0.5964 - val_accuracy: 0.7550\n",
      "Epoch 9/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5803 - accuracy: 0.7623 - val_loss: 0.5848 - val_accuracy: 0.7650\n",
      "Epoch 10/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5668 - accuracy: 0.7640 - val_loss: 0.5917 - val_accuracy: 0.7300\n",
      "Epoch 11/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5640 - accuracy: 0.7690 - val_loss: 0.5731 - val_accuracy: 0.7500\n",
      "Epoch 12/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5453 - accuracy: 0.7756 - val_loss: 0.5848 - val_accuracy: 0.7350\n",
      "Epoch 13/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5392 - accuracy: 0.7756 - val_loss: 0.5660 - val_accuracy: 0.7650\n",
      "Epoch 14/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5308 - accuracy: 0.7790 - val_loss: 0.5498 - val_accuracy: 0.7500\n",
      "Epoch 15/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5186 - accuracy: 0.7873 - val_loss: 0.6014 - val_accuracy: 0.7600\n",
      "Epoch 16/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5104 - accuracy: 0.7865 - val_loss: 0.6071 - val_accuracy: 0.7600\n",
      "Epoch 17/36\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5058 - accuracy: 0.7882 - val_loss: 0.5510 - val_accuracy: 0.7400\n",
      "Epoch 18/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5098 - accuracy: 0.7907 - val_loss: 0.5450 - val_accuracy: 0.7650\n",
      "Epoch 19/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5044 - accuracy: 0.7915 - val_loss: 0.5525 - val_accuracy: 0.7550\n",
      "Epoch 20/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4823 - accuracy: 0.8090 - val_loss: 0.5409 - val_accuracy: 0.7650\n",
      "Epoch 21/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4882 - accuracy: 0.8032 - val_loss: 0.5551 - val_accuracy: 0.7750\n",
      "Epoch 22/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4875 - accuracy: 0.7982 - val_loss: 0.5751 - val_accuracy: 0.7800\n",
      "Epoch 23/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4823 - accuracy: 0.8082 - val_loss: 0.5595 - val_accuracy: 0.7650\n",
      "Epoch 24/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4784 - accuracy: 0.8057 - val_loss: 0.5331 - val_accuracy: 0.7450\n",
      "Epoch 25/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4750 - accuracy: 0.8157 - val_loss: 0.5468 - val_accuracy: 0.7550\n",
      "Epoch 26/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4696 - accuracy: 0.8007 - val_loss: 0.5470 - val_accuracy: 0.7400\n",
      "Epoch 27/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4637 - accuracy: 0.8190 - val_loss: 0.5525 - val_accuracy: 0.7800\n",
      "Epoch 28/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4609 - accuracy: 0.8207 - val_loss: 0.5453 - val_accuracy: 0.7750\n",
      "Epoch 29/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4618 - accuracy: 0.8107 - val_loss: 0.5492 - val_accuracy: 0.7600\n",
      "Epoch 30/36\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4603 - accuracy: 0.8207 - val_loss: 0.5408 - val_accuracy: 0.7550\n",
      "Epoch 31/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4530 - accuracy: 0.8299 - val_loss: 0.5421 - val_accuracy: 0.7500\n",
      "Epoch 32/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4402 - accuracy: 0.8332 - val_loss: 0.5517 - val_accuracy: 0.7700\n",
      "Epoch 33/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4520 - accuracy: 0.8274 - val_loss: 0.5565 - val_accuracy: 0.7750\n",
      "Epoch 34/36\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4484 - accuracy: 0.8290 - val_loss: 0.5584 - val_accuracy: 0.7950\n",
      "Epoch 35/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4418 - accuracy: 0.8357 - val_loss: 0.5341 - val_accuracy: 0.8100\n",
      "Epoch 36/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4407 - accuracy: 0.8282 - val_loss: 0.5322 - val_accuracy: 0.8050\n"
     ]
    }
   ],
   "source": [
    "Model = finalModel()\n",
    "result = Model.fit(X_train, y_train, epochs = 36, batch_size=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd2c74",
   "metadata": {},
   "source": [
    "# 4 Results\n",
    "### 4.1 My basic model vs. my optimised model\n",
    "Now we have run the final model it is time to compare how a prototype model compares with my optimised model to do this I am going to run both models for 36 epochs on the previously unseen test data to see if and how much better my optimised model does against and a basic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0540cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicModel():\n",
    "    Model = models.Sequential()\n",
    "    Model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    Model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96c3957e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.8259 - accuracy: 0.5113 - val_loss: 0.8503 - val_accuracy: 0.5100\n",
      "Epoch 2/36\n",
      "24/24 [==============================] - 0s 614us/step - loss: 0.7941 - accuracy: 0.5288 - val_loss: 0.8215 - val_accuracy: 0.5200\n",
      "Epoch 3/36\n",
      "24/24 [==============================] - 0s 821us/step - loss: 0.7675 - accuracy: 0.5396 - val_loss: 0.7950 - val_accuracy: 0.5300\n",
      "Epoch 4/36\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.7423 - accuracy: 0.5588 - val_loss: 0.7685 - val_accuracy: 0.5350\n",
      "Epoch 5/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7181 - accuracy: 0.5721 - val_loss: 0.7435 - val_accuracy: 0.5450\n",
      "Epoch 6/36\n",
      "24/24 [==============================] - 0s 672us/step - loss: 0.6967 - accuracy: 0.5880 - val_loss: 0.7210 - val_accuracy: 0.5700\n",
      "Epoch 7/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6766 - accuracy: 0.6072 - val_loss: 0.6990 - val_accuracy: 0.5950\n",
      "Epoch 8/36\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6584 - accuracy: 0.6205 - val_loss: 0.6794 - val_accuracy: 0.6150\n",
      "Epoch 9/36\n",
      "24/24 [==============================] - 0s 518us/step - loss: 0.6426 - accuracy: 0.6472 - val_loss: 0.6615 - val_accuracy: 0.6300\n",
      "Epoch 10/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6282 - accuracy: 0.6555 - val_loss: 0.6446 - val_accuracy: 0.6700\n",
      "Epoch 11/36\n",
      "24/24 [==============================] - 0s 857us/step - loss: 0.6156 - accuracy: 0.6756 - val_loss: 0.6300 - val_accuracy: 0.6850\n",
      "Epoch 12/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6045 - accuracy: 0.6872 - val_loss: 0.6162 - val_accuracy: 0.7050\n",
      "Epoch 13/36\n",
      "24/24 [==============================] - 0s 559us/step - loss: 0.5946 - accuracy: 0.6997 - val_loss: 0.6037 - val_accuracy: 0.7150\n",
      "Epoch 14/36\n",
      "24/24 [==============================] - 0s 747us/step - loss: 0.5861 - accuracy: 0.7089 - val_loss: 0.5927 - val_accuracy: 0.7300\n",
      "Epoch 15/36\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.7148 - val_loss: 0.5817 - val_accuracy: 0.7400\n",
      "Epoch 16/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5715 - accuracy: 0.7214 - val_loss: 0.5725 - val_accuracy: 0.7500\n",
      "Epoch 17/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5659 - accuracy: 0.7248 - val_loss: 0.5648 - val_accuracy: 0.7600\n",
      "Epoch 18/36\n",
      "24/24 [==============================] - 0s 819us/step - loss: 0.5610 - accuracy: 0.7239 - val_loss: 0.5572 - val_accuracy: 0.7600\n",
      "Epoch 19/36\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5568 - accuracy: 0.7223 - val_loss: 0.5512 - val_accuracy: 0.7600\n",
      "Epoch 20/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5532 - accuracy: 0.7248 - val_loss: 0.5454 - val_accuracy: 0.7600\n",
      "Epoch 21/36\n",
      "24/24 [==============================] - 0s 726us/step - loss: 0.5502 - accuracy: 0.7264 - val_loss: 0.5410 - val_accuracy: 0.7550\n",
      "Epoch 22/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5477 - accuracy: 0.7281 - val_loss: 0.5370 - val_accuracy: 0.7650\n",
      "Epoch 23/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5453 - accuracy: 0.7314 - val_loss: 0.5328 - val_accuracy: 0.7700\n",
      "Epoch 24/36\n",
      "24/24 [==============================] - 0s 556us/step - loss: 0.5433 - accuracy: 0.7339 - val_loss: 0.5294 - val_accuracy: 0.7750\n",
      "Epoch 25/36\n",
      "24/24 [==============================] - 0s 761us/step - loss: 0.5415 - accuracy: 0.7348 - val_loss: 0.5264 - val_accuracy: 0.7850\n",
      "Epoch 26/36\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.5399 - accuracy: 0.7356 - val_loss: 0.5238 - val_accuracy: 0.7900\n",
      "Epoch 27/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5387 - accuracy: 0.7314 - val_loss: 0.5216 - val_accuracy: 0.7900\n",
      "Epoch 28/36\n",
      "24/24 [==============================] - 0s 645us/step - loss: 0.5375 - accuracy: 0.7314 - val_loss: 0.5197 - val_accuracy: 0.7850\n",
      "Epoch 29/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5365 - accuracy: 0.7331 - val_loss: 0.5178 - val_accuracy: 0.7850\n",
      "Epoch 30/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5356 - accuracy: 0.7306 - val_loss: 0.5160 - val_accuracy: 0.7850\n",
      "Epoch 31/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5348 - accuracy: 0.7298 - val_loss: 0.5145 - val_accuracy: 0.7850\n",
      "Epoch 32/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5340 - accuracy: 0.7314 - val_loss: 0.5131 - val_accuracy: 0.7900\n",
      "Epoch 33/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5334 - accuracy: 0.7298 - val_loss: 0.5120 - val_accuracy: 0.7800\n",
      "Epoch 34/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5328 - accuracy: 0.7306 - val_loss: 0.5108 - val_accuracy: 0.7800\n",
      "Epoch 35/36\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5322 - accuracy: 0.7314 - val_loss: 0.5097 - val_accuracy: 0.7850\n",
      "Epoch 36/36\n",
      "24/24 [==============================] - 0s 814us/step - loss: 0.5317 - accuracy: 0.7331 - val_loss: 0.5088 - val_accuracy: 0.7800\n"
     ]
    }
   ],
   "source": [
    "Model = basicModel()\n",
    "basicResult = Model.fit(X_train, y_train, epochs = 36, batch_size=50, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10cac7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalModel():\n",
    "    Model = models.Sequential()\n",
    "    Model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    Model.add(layers.Dropout(0.15))\n",
    "    Model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    Model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "943ecdb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "24/24 [==============================] - 1s 13ms/step - loss: 1.4034 - accuracy: 0.7073 - val_loss: 1.1455 - val_accuracy: 0.7850\n",
      "Epoch 2/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 1.0842 - accuracy: 0.7423 - val_loss: 0.9617 - val_accuracy: 0.7850\n",
      "Epoch 3/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.9105 - accuracy: 0.7423 - val_loss: 0.8275 - val_accuracy: 0.7850\n",
      "Epoch 4/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.7846 - accuracy: 0.7565 - val_loss: 0.7160 - val_accuracy: 0.7900\n",
      "Epoch 5/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.7210 - accuracy: 0.7565 - val_loss: 0.6610 - val_accuracy: 0.7900\n",
      "Epoch 6/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6729 - accuracy: 0.7573 - val_loss: 0.6357 - val_accuracy: 0.7650\n",
      "Epoch 7/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6310 - accuracy: 0.7631 - val_loss: 0.6275 - val_accuracy: 0.7750\n",
      "Epoch 8/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5993 - accuracy: 0.7673 - val_loss: 0.5643 - val_accuracy: 0.7900\n",
      "Epoch 9/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5859 - accuracy: 0.7631 - val_loss: 0.5425 - val_accuracy: 0.7950\n",
      "Epoch 10/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5666 - accuracy: 0.7723 - val_loss: 0.5496 - val_accuracy: 0.7950\n",
      "Epoch 11/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5639 - accuracy: 0.7606 - val_loss: 0.5251 - val_accuracy: 0.8050\n",
      "Epoch 12/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5496 - accuracy: 0.7690 - val_loss: 0.5209 - val_accuracy: 0.8050\n",
      "Epoch 13/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5375 - accuracy: 0.7748 - val_loss: 0.5213 - val_accuracy: 0.7850\n",
      "Epoch 14/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5270 - accuracy: 0.7790 - val_loss: 0.5223 - val_accuracy: 0.7950\n",
      "Epoch 15/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5189 - accuracy: 0.7740 - val_loss: 0.4888 - val_accuracy: 0.8100\n",
      "Epoch 16/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5153 - accuracy: 0.7898 - val_loss: 0.4983 - val_accuracy: 0.8000\n",
      "Epoch 17/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5162 - accuracy: 0.7865 - val_loss: 0.5141 - val_accuracy: 0.8000\n",
      "Epoch 18/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.5062 - accuracy: 0.7890 - val_loss: 0.5233 - val_accuracy: 0.8150\n",
      "Epoch 19/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4967 - accuracy: 0.7982 - val_loss: 0.5075 - val_accuracy: 0.8100\n",
      "Epoch 20/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4967 - accuracy: 0.7923 - val_loss: 0.5009 - val_accuracy: 0.8100\n",
      "Epoch 21/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4983 - accuracy: 0.7923 - val_loss: 0.5250 - val_accuracy: 0.8000\n",
      "Epoch 22/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4786 - accuracy: 0.7990 - val_loss: 0.4799 - val_accuracy: 0.8150\n",
      "Epoch 23/36\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.4852 - accuracy: 0.8073 - val_loss: 0.4826 - val_accuracy: 0.8200\n",
      "Epoch 24/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4750 - accuracy: 0.8048 - val_loss: 0.4771 - val_accuracy: 0.8200\n",
      "Epoch 25/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4702 - accuracy: 0.8107 - val_loss: 0.4942 - val_accuracy: 0.8100\n",
      "Epoch 26/36\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.4784 - accuracy: 0.8082 - val_loss: 0.4865 - val_accuracy: 0.8050\n",
      "Epoch 27/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4684 - accuracy: 0.8198 - val_loss: 0.4676 - val_accuracy: 0.8150\n",
      "Epoch 28/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4591 - accuracy: 0.8190 - val_loss: 0.4774 - val_accuracy: 0.8300\n",
      "Epoch 29/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4538 - accuracy: 0.8140 - val_loss: 0.5041 - val_accuracy: 0.8000\n",
      "Epoch 30/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4529 - accuracy: 0.8182 - val_loss: 0.4856 - val_accuracy: 0.8250\n",
      "Epoch 31/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4552 - accuracy: 0.8365 - val_loss: 0.4937 - val_accuracy: 0.7850\n",
      "Epoch 32/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4397 - accuracy: 0.8274 - val_loss: 0.5038 - val_accuracy: 0.7950\n",
      "Epoch 33/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4513 - accuracy: 0.8224 - val_loss: 0.4631 - val_accuracy: 0.8300\n",
      "Epoch 34/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4417 - accuracy: 0.8215 - val_loss: 0.4561 - val_accuracy: 0.8200\n",
      "Epoch 35/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4374 - accuracy: 0.8265 - val_loss: 0.4813 - val_accuracy: 0.8300\n",
      "Epoch 36/36\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.4483 - accuracy: 0.8390 - val_loss: 0.4811 - val_accuracy: 0.8150\n"
     ]
    }
   ],
   "source": [
    "Model = finalModel()\n",
    "finalResult = Model.fit(X_train, y_train, epochs = 36, batch_size=50, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4780ced8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Compare results using a graph\n",
    "basicValAcc = basicResult.history['val_accuracy']\n",
    "finalValAcc = finalResult.history['val_accuracy']\n",
    "epochs = range(1, len(finalValAcc) + 1)\n",
    "plt.plot(epochs, basicValAcc, 'b', label = 'Basic result')\n",
    "plt.plot(epochs, finalValAcc, 'g', label = 'Final result')\n",
    "plt.title('Basic vs. final result')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05742648",
   "metadata": {},
   "source": [
    "Finally as you could see from the results and the graph the final model does better than the basic model when testing on new data. The basic model ends up at 0.78 while the final model ends up at 0.815 the final model ends up being 0.035 better which is quite small but this model does give an exceptible rate of accuracy for someone to be sure if their wine is good or bad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
